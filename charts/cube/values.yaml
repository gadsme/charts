## Override the name
##
nameOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

##  Labels to add to all deployed objects
##
commonLabels: {}

## Annotations to add to all deployed objects
##
commonAnnotations: {}

## Extra environment variables to pass on to the pod. The value is evaluated as a template
## e.g:
## extraEnvVars:
##   - name: FOO
##     value: "bar"
##
extraEnvVars: []

## Name of a Config Map containing extra environment variables to pass on to the pod
##
extraEnvVarsFromConfigMap:

## Name of a Secret containing extra environment variables to pass on to the pod
##
extraEnvVarsFromSecret:

image:
  ## Docker image repository
  ##
  repository: cubejs/cube

  ## Docker image tag.
  ##
  tag:

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent

  ## Specify a imagePullSecrets
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  ##
  pullSecrets: []

## Pod Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
## Some admission policies only allow use of non root containers
securityContext:
  enabled: false
  # fsGroup: 1001
  # runAsUser: 1001


config:
  ## The port for a Cube deployment to listen to API connections on
  ##
  apiPort: 4000

  ## The port to listen to MySQL-compatible connections on
  ##
  sqlPort:

  ## The port to listen to Postgres-compatible connections on
  ##
  pgSqlPort:

  ## The username to access the SQL api
  ##
  sqlUser:

  ## The password to access the SQL api
  ##
  sqlPassword:
  # apiPasswordFromSecret:
  #   name:
  #   key:

  ## If true, enables development mode
  ##
  devMode: false

  ## If true, enables debug logging
  ##
  debug: false

  ## The logging level for Cube
  ##
  logLevel: "warn"

  ## If true, then send telemetry to Cube
  ##
  telemetry: false

  ## The secret key used to sign and verify JWTs. Generated on project scaffold
  ##
  apiSecret:
  # apiSecretFromSecret:
  #   name:
  #   key:

  ## The secret key used to enable playground and system apis
  ##
  playgroundAuthSecret:
  # apiSecretFromSecret:
  #   name:
  #   key:

  ## The path where Cube loads schemas from. Defaults to schema
  ##
  schemaPath:

  ## An application ID used to uniquely identify the Cube deployment. Can be different for multitenant setups
  ## Defaults to cubejs
  ##
  app:

  ## If true, this instance of Cube will only query rollup pre-aggregations. Defaults to false
  ##
  rollupOnly:

  ## A comma-separated list of timezones to schedule refreshes for.
  ##
  scheduledRefreshTimezones:

  ## How many pre-aggregations refresh worker will build in parallel. Please note changing this param doesn't change queue concurrency and it should be adjusted accordingly
  ##
  scheduledRefreshConcurrency:

  ## The schema name to use for storing pre-aggregations.
  ## Defaults to dev_pre_aggregations/prod_pre_aggregations for development/production mode
  ##
  preAggregationsSchema:

  ## If true, then use WebSocket for data fetching. Defaults to true
  ##
  webSockets:

  ## The cache and queue driver to use for the Cube deployment. Defaults to cubestore
  ##
  cacheAndQueueDriver:

  ## The number of concurrent connections each query queue has to the database
  ##
  concurrency:

  ## The name of the Amazon SNS or Google Cloud Pub/Sub topic
  ## Defaults to <process.env.CUBEJS_APP>-process if undefined, and finally cubejs-process
  ##
  topicName:

  ## The number of seconds without a touch before pre-aggregation is considered orphaned and marked for removal.
  ##
  touchPreAggTimeout:

  ## If true, it enables dropping pre-aggregations that Refresh Worker doesn't touch within touchPreAggTimeout.
  ## Pre-aggregations are touched whenever they are rebuilt or a Refresh Worker checks its freshness.
  ## The first drop will be initiated when the Refresh Worker is able to check freshness for every scheduledRefresh: true pre-aggregation.
  ## If you have multiple Refresh Workers with different data model versions sharing the same Cube Store cluster,
  ## then touches from both refresh workers are respected.
  ##
  dropPreAggWithoutTouch:

  ## Init containers to run before starting the main container
  ## We can use an empty volume and volume mounts to copy data
  ## from a remote location like s3 to the local volume
  ##
  initContainers: []
  # - name: aws-init-container
  #   image: amazon/aws-cli:latest
  #   command: ['sh', '-c', 'aws s3 cp s3://your-s3-bucket/your-data /path/to/volume/mount']
  #   volumeMounts:
  #   - name: data-volume
  #     mountPath: /path/to/volume/mount

  ## The config volumes
  ## Will be used to both api and worker
  volumes: []
  ## In case you want to use configMap
  # - name: schema
  #   configMap:
  #     name: schema
  ## In case you want to use init container
  # - name: data-volume
  #   emptyDir: {}

  ## The config volumeMounts
  ## Will be used to both api and worker
  volumeMounts: []
  ## In case you want to use configMap
  # - name: schema
  #   readOnly: true
  #   mountPath: /cube/conf/schema
  ## In case you want to use init container
  # - name: data-volume
  #   readOnly: true
  #   mountPath: /cube/conf/schema
  #   subPath: model

redis:
  ## The host URL for a Redis server
  ##
  url:

  ## The password used to connect to the Redis server
  ##
  password:
  # passwordFromSecret:
  #   name:
  #   key:

  ## If true, then the connection to the Redis server is protected by TLS authentication. Defaults to false
  ##
  tls:

  ## The minimum number of connections to keep active in the Redis connection pool for a single appId (tenant).
  ## Must be lower than poolMax. Defaults to 2
  ##
  poolMin:

  ## The maximum number of connections to keep active in the Redis connection pool for a single appId (tenant).
  ## Must be higher than poolMin. Defaults to 1000
  ##
  poolMax:

  ## Use ioredis instead of redis. Defaults to false
  ##
  useIoRedis:

jwt:
  ## A valid URL to a JSON Web Key Sets (JWKS)
  ##
  jwkUrl:

  ## The secret key used to sign and verify JWTs. Generated on project scaffold
  ##
  key:
  # keyFromSecret:
  #   name:
  #   key:

  ## An audience value which will be used to enforce the aud claim from inbound JWTs
  ##
  audience:

  ## An issuer value which will be used to enforce the iss claim from inbound JWTs
  ##
  issuer:

  ## A subject value which will be used to enforce the sub claim from inbound JWTs
  ##
  subject:

  ## Any supported algorithm for decoding JWTs
  ##
  algs:

  ## A namespace within the decoded JWT under which any custom claims can be found
  ##
  claimsNamespace:

## Datasource configuration
##
datasources:
  ## Default datasource
  default:
    ## A database type supported by Cube
    ##
    type:

    ## The URL for a database
    ##
    url:

    ## The host URL for a database
    ##
    host:

    ## The port for the database connection
    ##
    port:

    ## The schema within the database to connect to
    ##
    schema:

    ## The name of the database to connect to
    ##
    name:

    ## The username used to connect to the database
    ##
    user:

    ## The password used to connect to the database
    ##
    pass:
    # passFromSecret:
    #   name:
    #   key:

    ## A domain name within the database to connect to
    ##
    domain:

    ## The path to a Unix socket for a MySQL database
    ##
    socketPath:

    ## The catalog within the database to connect to
    ##
    catalog:

    ## The maximum number of connections to keep active in the database connection pool
    ##
    maxPool:

    ## A number in seconds or a duration string
    ##
    queryTimeout:

    ssl:
      ## If true, enables SSL encryption for database connections from Cube
      ##
      enabled: false

      ## If true, verifies the CA chain with the system's built-in CA chain
      ##
      rejectUnAuthorized:
      ## The contents of a CA bundle in PEM format, or a path to one.
      ## For more information, check the options.ca property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      ca:

      ## The contents of an SSL certificate in PEM format, or a path to one.
      ## For more information, check the options.cert property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      cert:

      ## The contents of a private key in PEM format, or a path to one.
      ## For more information, check the options.key property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      key:

      ## The ciphers used by the SSL certificate.
      ## For more information, check the options.ciphers property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      ciphers:

      ## The server name for the SNI TLS extension.
      ## For more information, check the options.servername property for TLS Connections in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      serverName:

      ## The passphrase used to encrypt the SSL private key.
      ## For more information, check the options.passphrase property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      passPhrase:

    ## Export Bucket configuration
    ##
    export:
      ## The name of a bucket in cloud storage to store the database export snapshots.
      ##
      name:

      ## The cloud provider where the bucket is hosted (gcp, s3)
      ##
      type:

      ## The name of the integration used in the database. Only required when using Snowflake and Google Cloud Storage
      ##
      integration:

      gcs:
        ## A Base64 encoded JSON key file for connecting to Google Cloud
        ##
        credentials:
        # credentialsFromSecret:
        #   name:
        #   key:

      aws:
        ## The AWS Access Key ID to use for the export bucket.
        ##
        key:

        ## The AWS Secret Access Key to use for the export bucket.
        ##
        secret:
        # secretFromSecret:
        #   name:
        #   key:

        ## The AWS region of the export bucket.
        ##
        region:

      redshift:
        ## An ARN of an AWS IAM role with permission to write to the configured bucket (see export.name).
        arn:

    athena:
      ## The AWS Access Key ID to use for database connections
      ##
      key:
      # keyFromSecret:
      #   name:
      #   key:

      ## The AWS region of the Cube deployment
      ##
      region:

      ## The S3 path to store query results made by the Cube deployment
      ##
      s3OutputLocation:

      ## The AWS Secret Access Key to use for database connections
      ##
      secret:
      # secretFromSecret:
      #   name:
      #   key:

      ## The name of the workgroup in which the query is being started
      ##
      workgroup:

      ## The name of the catalog to use by default
      ##
      catalog:

    bigquery:
      ## The Google BigQuery project ID to connect to
      ##
      projectId:

      ## The Google BigQuery dataset location to connect to
      ##
      location:

      ## A Base64 encoded JSON key file for connecting to Google BigQuery
      ##
      credentials:
      # credentialsFromSecret:
      #   name:
      #   key:

      ## The path to a JSON key file for connecting to Google BigQuery
      ##
      keyFile:

    hive:
      ## The version of the CDH instance for Apache Hive
      ##
      cdhVersion:

      ## The version of Thrift Server for Apache Hive
      ##
      thriftVersion:

      ## The type of Apache Hive server
      ##
      type:

      ## The version of Apache Hive
      version:

    snowFlake:
      ## The Snowflake account ID to use when connecting to the database
      ##
      account:

      ## The Snowflake region to use when connecting to the database
      ##
      region:

      ## The Snowflake role to use when connecting to the database
      ##
      role:

      ## The Snowflake warehouse to use when connecting to the database
      ##
      warehouse:

      ## If true, keep the Snowflake connection alive indefinitely
      ##
      clientSessionKeepAlive:

      ## The type of authenticator to use with Snowflake.
      ## Use SNOWFLAKE with username/password, or SNOWFLAKE_JWT with key pairs.
      ## Defaults to SNOWFLAKE
      ##
      authenticator:

      ## The path to the private RSA key folder
      ##
      privateKeyPath:

      ## The password for the private RSA key. Only required for encrypted keys
      ##
      privateKeyPass:

    databricks:
      ## The URL for a JDBC connection
      ##
      url:

      ## Whether or not to accept the license terms for the Databricks JDBC driver
      ##
      acceptPolicy:

      ## The personal access token used to authenticate the Databricks connection
      ##
      token:

      ## Databricks catalog name
      ##
      catalog:

    clickhouse:
      ## Whether the ClickHouse user has read-only access or not
      ##
      readonly:

    firebolt:
      ## Account name
      ##
      account:

      ## Engine name to connect to
      ##
      engineName:

      ## Firebolt API endpoint. Used for authentication
      ##
      apiEndpoint:

    presto:
      ## The catalog within Presto to connect to
      ##
      catalog:

    elasticsearch:
      ## By default, queries return data in JDBC format, but you can also return data in standard Elasticsearch JDBC, JSON, CSV, YAML or raw formats (only JSON and JDBC are currently supported)
      ##
      queryFormat:

      ## If true, then use the Open Distro for Elasticsearch
      ##
      openDistro:

      ## ID of the API key from elastic.co
      ##
      apiKeyId:

      ## Value of the API key from elastic.co
      ##
      apiKeyKey:

    trino:
      ## The catalog within Trino to connect to
      ##
      catalog:
## Cubestore configuration
##
cubestore:
  ## The hostname of the Cube Store deployment
  ##
  host:

  ## The port of the Cube Store deployment
  ##
  port: 3030

api:
  ## Service account for cube api to use
  ##
  serviceAccount:
    create: false
    name: ""
    automountServiceAccountToken: true
    annotations: {}

  apiCount: 1

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Topology spread constraint for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  spreadConstraints: []

  ## Define resources requests and limits for single Pods.
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ##  Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}

  ## Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}

  ## Tolerations for pod assignment
  ##
  tolerations: {}

  ## Node selector for pod assignment
  ##
  nodeSelector: {}

worker:
  ## Set to true to enable a separate refresh worker
  ##
  enabled: true

  ## Service account for cube worker to use
  ##
  serviceAccount:
    create: false
    name: ""
    automountServiceAccountToken: true
    annotations: {}

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## topology spread constraint for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  spreadConstraints: []

  ## Define resources requests and limits for single Pods.
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ##  Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}

  ## Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}

  ## Tolerations for pod assignment
  ##
  tolerations: {}

  ## Node selector for pod assignment
  ##
  nodeSelector: {}

## Configure the ingress resource that allows you to access the
## Cube installation. Set up the URL
## ref: http://kubernetes.io/docs/user-guide/ingress/
##
ingress:
  ## Set to true to enable ingress record generation
  ##
  enabled: false

  ## When the ingress is enabled, a host pointing to this will be created
  ##
  hostname: cube.local

  ## The Path to Cube. You may need to set this to '/*' in order to use this
  ## with ALB ingress controllers.
  ##
  path: /

  ## Ingress Path type
  ##
  pathType: ImplementationSpecific

  ## Ingress Class name
  ##
  ingressClassName:

  ## Ingress annotations done as key:value pairs
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ##
  annotations: {}

  ## Enable TLS configuration for the hostname defined at ingress.hostname parameter
  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}
  ## You can use the ingress.secrets parameter to create this TLS secret or relay on cert-manager to create it
  ##
  tls: false
