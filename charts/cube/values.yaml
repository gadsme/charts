## Override the name
##
nameOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

##  Labels to add to all deployed objects
##
commonLabels: {}

## Annotations to add to all deployed objects
##
commonAnnotations: {}

## Extra environment variables to pass on to all pods. The value is evaluated as a template
## e.g:
## extraEnvVars:
##   - name: FOO
##     value: "bar"
##
extraEnvVars: []

## Name of a Config Map containing extra environment variables to pass on to all pods
##
extraEnvVarsFromConfigMap:

## Name of a Secret containing extra environment variables to pass on to all pods
##
extraEnvVarsFromSecret:

image:
  ## Docker image repository
  ##
  repository: cubejs/cube

  ## Docker image tag.
  ##
  tag:

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent

  ## Specify a imagePullSecrets
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  ##
  pullSecrets: []

## Pod Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
## Some admission policies only allow use of non root containers
securityContext:
  enabled: false
  # fsGroup: 1001
  # runAsUser: 1001


config:
  ## The port for a Cube deployment to listen to API connections on
  ##
  apiPort: 4000

  ## The port to listen to Postgres-compatible connections on
  ##
  pgSqlPort:

  ## The username to access the SQL api
  ##
  sqlUser:

  ## The password to access the SQL api
  ##
  sqlPassword:
  # apiPasswordFromSecret:
  #   name:
  #   key:

  ## If true, enables development mode
  ##
  devMode: false

  ## If true, enables debug logging
  ##
  debug: false

  ## The logging level for Cube
  ##
  logLevel: "warn"

  ## If true, then send telemetry to Cube
  ##
  telemetry: false

  ## The secret key used to sign and verify JWTs. Generated on project scaffold
  ##
  apiSecret:
  # apiSecretFromSecret:
  #   name:
  #   key:

  ## The secret key used to enable playground and system apis
  ##
  playgroundAuthSecret:
  # apiSecretFromSecret:
  #   name:
  #   key:

  ## The path where Cube loads schemas from. Defaults to schema
  ##
  schemaPath: /cube/conf/model

  ## An application ID used to uniquely identify the Cube deployment. Can be different for multitenant setups
  ## Defaults to cubejs
  ##
  app:

  ## If true, this instance of Cube will only query rollup pre-aggregations. Defaults to false
  ##
  rollupOnly:

  ## A comma-separated list of timezones to schedule refreshes for.
  ##
  scheduledRefreshTimezones:

  ## How many pre-aggregations refresh worker will build in parallel. Please note changing this param doesn't change queue concurrency and it should be adjusted accordingly
  ##
  scheduledRefreshConcurrency:

  ## The schema name to use for storing pre-aggregations.
  ## Defaults to dev_pre_aggregations/prod_pre_aggregations for development/production mode
  ##
  preAggregationsSchema:

  ## If true, then use WebSocket for data fetching. Defaults to true
  ##
  webSockets:

  ## The cache and queue driver to use for the Cube deployment. Defaults to cubestore
  ##
  cacheAndQueueDriver:

  ## The number of concurrent connections each query queue has to the database
  ##
  concurrency:

  ## The name of the Amazon SNS or Google Cloud Pub/Sub topic
  ## Defaults to <process.env.CUBEJS_APP>-process if undefined, and finally cubejs-process
  ##
  topicName:

  ## The number of seconds without a touch before pre-aggregation is considered orphaned and marked for removal.
  ##
  touchPreAggTimeout:

  ## If true, it enables dropping pre-aggregations that Refresh Worker doesn't touch within touchPreAggTimeout.
  ## Pre-aggregations are touched whenever they are rebuilt or a Refresh Worker checks its freshness.
  ## The first drop will be initiated when the Refresh Worker is able to check freshness for every scheduledRefresh: true pre-aggregation.
  ## If you have multiple Refresh Workers with different data model versions sharing the same Cube Store cluster,
  ## then touches from both refresh workers are respected.
  ##
  dropPreAggWithoutTouch:

  ## Init containers to run before starting the main container
  ## We can use an empty volume and volume mounts to copy data
  ## from a remote location like s3 to the local volume
  ##
  initContainers: []
  # - name: aws-init-container
  #   image: amazon/aws-cli:latest
  #   command: ['sh', '-c', 'aws s3 cp s3://your-s3-bucket/your-data /path/to/volume/mount']
  #   volumeMounts:
  #   - name: data-volume
  #     mountPath: /path/to/volume/mount

  gitSync:
    image:
      repository: registry.k8s.io/git-sync/git-sync
      tag: v4.1.0
      pullPolicy: IfNotPresent
    enabled: false

    # git repo clone url
    # ssh example: git@github.com:cube-js/cube.git
    # https example: https://github.com/cube-js/cube.git
    repo: https://github.com/cube-js/cube.git
    branch: v1.1.7
    rev: HEAD
    # The git revision (branch, tag, or hash) to check out, v4 only
    ref: v1.1.7
    depth: 1
    # the number of consecutive failures allowed before aborting
    maxFailures: 0
    # subpath within the repo where schemas are located
    # should be "" if schemas are at repo root
    subPath: "tests/schemas"
    # if your repo needs a user name password
    # you can load them to a k8s secret like the one below
    #   ---
    #   apiVersion: v1
    #   kind: Secret
    #   metadata:
    #     name: git-credentials
    #   data:
    #     # For git-sync v3
    #     GIT_SYNC_USERNAME: <base64_encoded_git_username>
    #     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
    #     # For git-sync v4
    #     GITSYNC_USERNAME: <base64_encoded_git_username>
    #     GITSYNC_PASSWORD: <base64_encoded_git_password>
    # and specify the name of the secret below
    #
    # credentialsSecret: git-credentials
    #
    #
    # If you are using an ssh clone url, you can load
    # the ssh private key to a k8s secret like the one below
    #   ---
    #   apiVersion: v1
    #   kind: Secret
    #   metadata:
    #     name: airflow-ssh-secret
    #   data:
    #     # key needs to be gitSshKey
    #     gitSshKey: <base64_encoded_data>
    # and specify the name of the secret below
    # sshKeySecret: airflow-ssh-secret
    #
    # Or set sshKeySecret with your key
    # sshKey: |-
    #   -----BEGIN {OPENSSH PRIVATE KEY}-----
    #   ...
    #   -----END {OPENSSH PRIVATE KEY}-----
    #
    # If you are using an ssh private key, you can additionally
    # specify the content of your known_hosts file, example:
    #
    # knownHosts: |
    #    <host1>,<ip1> <key1>
    #    <host2>,<ip2> <key2>

    # interval between git sync attempts in seconds
    # high values are more likely to cause schemas to become out of sync between different components
    # low values cause more traffic to the remote git repository
    # Go-style duration string (e.g. "100ms" or "0.1s" = 100ms).
    # For backwards compatibility, wait will be used if it is specified.
    period: 5s
    wait: ~
    # add variables from secret into gitSync containers, such proxy-config
    envFrom: ~
    # envFrom: |
    #   - secretRef:
    #       name: 'proxy-config'

    containerName: git-sync
    uid: 65533

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 65533
    #  runAsGroup: 0

    securityContexts:
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Mount additional volumes into git-sync. It can be templated like in the following example:
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumeMounts: []
    env: []
    # Supported env vars for gitsync can be found at https://github.com/kubernetes/git-sync
    # - name: ""
    #   value: ""

    # Configuration for empty dir volume
    # emptyDirConfig:
    #   sizeLimit: 1Gi
    #   medium: Memory

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## The config volumes
  ## Will be used to both api and worker
  volumes: []
  ## In case you want to use configMap
  # - name: schema
  #   configMap:
  #     name: schema
  ## In case you want to use init container
  # - name: data-volume
  #   emptyDir: {}

  ## The config volumeMounts
  ## Will be used to both api and worker
  volumeMounts: []
  ## In case you want to use configMap
  # - name: schema
  #   readOnly: true
  #   mountPath: /cube/conf/schema
  ## In case you want to use init container
  # - name: data-volume
  #   readOnly: true
  #   mountPath: /cube/conf/schema
  #   subPath: model

redis:
  ## The host URL for a Redis server
  ##
  url:

  ## The password used to connect to the Redis server
  ##
  password:
  # passwordFromSecret:
  #   name:
  #   key:

  ## If true, then the connection to the Redis server is protected by TLS authentication. Defaults to false
  ##
  tls:

  ## The minimum number of connections to keep active in the Redis connection pool for a single appId (tenant).
  ## Must be lower than poolMax. Defaults to 2
  ##
  poolMin:

  ## The maximum number of connections to keep active in the Redis connection pool for a single appId (tenant).
  ## Must be higher than poolMin. Defaults to 1000
  ##
  poolMax:

  ## Use ioredis instead of redis. Defaults to false
  ##
  useIoRedis:

jwt:
  ## A valid URL to a JSON Web Key Sets (JWKS)
  ##
  jwkUrl:

  ## The secret key used to sign and verify JWTs. Generated on project scaffold
  ##
  key:
  # keyFromSecret:
  #   name:
  #   key:

  ## An audience value which will be used to enforce the aud claim from inbound JWTs
  ##
  audience:

  ## An issuer value which will be used to enforce the iss claim from inbound JWTs
  ##
  issuer:

  ## A subject value which will be used to enforce the sub claim from inbound JWTs
  ##
  subject:

  ## Any supported algorithm for decoding JWTs
  ##
  algs:

  ## A namespace within the decoded JWT under which any custom claims can be found
  ##
  claimsNamespace:

## Datasource configuration
##
datasources:
  ## Default datasource
  default:
    ## A database type supported by Cube
    ##
    type:

    ## The URL for a database
    ##
    url:

    ## The host URL for a database
    ##
    host:

    ## The port for the database connection
    ##
    port:

    ## The schema within the database to connect to
    ##
    schema:

    ## The name of the database to connect to
    ##
    name:

    ## The username used to connect to the database
    ##
    user:

    ## The password used to connect to the database
    ##
    pass:
    # passFromSecret:
    #   name:
    #   key:

    ## A domain name within the database to connect to
    ##
    domain:

    ## The path to a Unix socket for a MySQL database
    ##
    socketPath:

    ## The catalog within the database to connect to
    ##
    catalog:

    ## The maximum number of connections to keep active in the database connection pool
    ##
    maxPool:

    ## A number in seconds or a duration string
    ##
    queryTimeout:

    ## Force fetching of columns by ordinal positions. Certain data-providers (e.g., Redshift) do not guarantee columns in the same order on each request (e.g., SELECT * FROM foo). This flag ensures that columns will be fetched in proper order for pre-aggregation generation.
    ##
    fetchColumnsByOrdinalPosition:

    ssl:
      ## If true, enables SSL encryption for database connections from Cube
      ##
      enabled: false

      ## If true, verifies the CA chain with the system's built-in CA chain
      ##
      rejectUnAuthorized:
      ## The contents of a CA bundle in PEM format, or a path to one.
      ## For more information, check the options.ca property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      ca:

      ## The contents of an SSL certificate in PEM format, or a path to one.
      ## For more information, check the options.cert property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      cert:

      ## The contents of a private key in PEM format, or a path to one.
      ## For more information, check the options.key property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      key:

      ## The ciphers used by the SSL certificate.
      ## For more information, check the options.ciphers property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      ciphers:

      ## The server name for the SNI TLS extension.
      ## For more information, check the options.servername property for TLS Connections in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      serverName:

      ## The passphrase used to encrypt the SSL private key.
      ## For more information, check the options.passphrase property for TLS Secure Contexts in the Node.js documentation
      ## https://nodejs.org/docs/latest/api/tls.html#tls_tls_createsecurecontext_options
      ##
      passPhrase:

    ## Export Bucket configuration
    ##
    export:
      ## The name of a bucket in cloud storage to store the database export snapshots.
      ##
      name:

      ## The cloud provider where the bucket is hosted (gcp, s3)
      ##
      type:

      ## The name of the integration used in the database. Only required when using Snowflake and Google Cloud Storage
      ##
      integration:

      gcs:
        ## A Base64 encoded JSON key file for connecting to Google Cloud
        ##
        credentials:
        # credentialsFromSecret:
        #   name:
        #   key:

      aws:
        ## The AWS Access Key ID to use for the export bucket.
        ##
        key:
        # keyFromSecret:
        #   name:
        #   key:

        ## The AWS Secret Access Key to use for the export bucket.
        ##
        secret:
        # secretFromSecret:
        #   name:
        #   key:

        ## The AWS region of the export bucket.
        ##
        region:

      redshift:
        ## An ARN of an AWS IAM role with permission to write to the configured bucket (see export.name).
        arn:

    athena:
      ## The AWS Access Key ID to use for database connections
      ##
      key:
      # keyFromSecret:
      #   name:
      #   key:

      ## The AWS region of the Cube deployment
      ##
      region:

      ## The S3 path to store query results made by the Cube deployment
      ##
      s3OutputLocation:

      ## The AWS Secret Access Key to use for database connections
      ##
      secret:
      # secretFromSecret:
      #   name:
      #   key:

      ## The name of the workgroup in which the query is being started
      ##
      workgroup:

      ## The name of the catalog to use by default
      ##
      catalog:

    bigquery:
      ## The Google BigQuery project ID to connect to
      ##
      projectId:

      ## The Google BigQuery dataset location to connect to
      ##
      location:

      ## A Base64 encoded JSON key file for connecting to Google BigQuery
      ##
      credentials:
      # credentialsFromSecret:
      #   name:
      #   key:

      ## The path to a JSON key file for connecting to Google BigQuery
      ##
      keyFile:

    dubckdb:
      ## The maximum memory limit for DuckDB. Equivalent to SET memory_limit=<MEMORY_LIMIT>. Default is 75% of available RAM
      ##
      memoryLimit:

      ## The default search schema
      ##
      schema:

      ## The service token to use for connections to MotherDuck
      ##
      motherduckToken:

      ## The database filepath to use for connection to a local
      ##
      databasePath:

      s3:
        ## The Access Key ID to use for database
        ##
        accessKeyId:

        ## The Secret Access Key to use for database
        ##
        secretAccessKey:
        # secretAccessKeyFromSecret:
        #   name:
        #   key:

        ## The S3 endpoint
        ##
        endpoint:

        ## The region of the bucket
        ##
        region:

        ## Use SSL for connection
        ##
        useSSL:

        ## To choose the S3 URL style(vhost or path)
        ##
        urlStyle:

        ## The token for the S3 session
        ##
        sessionToken:

    hive:
      ## The version of the CDH instance for Apache Hive
      ##
      cdhVersion:

      ## The version of Thrift Server for Apache Hive
      ##
      thriftVersion:

      ## The type of Apache Hive server
      ##
      type:

      ## The version of Apache Hive
      version:

    materialize:
      ## The name of the Materialize cluster to connect to
      ##
      cluster: 

    snowFlake:
      ## The Snowflake account ID to use when connecting to the database
      ##
      account:

      ## The Snowflake region to use when connecting to the database
      ##
      region:

      ## The Snowflake role to use when connecting to the database
      ##
      role:

      ## The Snowflake warehouse to use when connecting to the database
      ##
      warehouse:

      ## If true, keep the Snowflake connection alive indefinitely
      ##
      clientSessionKeepAlive:

      ## The type of authenticator to use with Snowflake.
      ## Use SNOWFLAKE with username/password, or SNOWFLAKE_JWT with key pairs.
      ## Defaults to SNOWFLAKE
      ##
      authenticator:

      ## The path to the private RSA key folder
      ##
      privateKeyPath:

      ## The password for the private RSA key. Only required for encrypted keys
      ##
      privateKeyPass:

    databricks:
      ## The URL for a JDBC connection
      ##
      url:

      ## Whether or not to accept the license terms for the Databricks JDBC driver
      ##
      acceptPolicy:

      ## The personal access token used to authenticate the Databricks connection
      ##
      token:

      ## Databricks catalog name
      ##
      catalog:

    clickhouse:
      ## Whether the ClickHouse user has read-only access or not
      ##
      readonly:

    firebolt:
      ## Account name
      ##
      account:

      ## Engine name to connect to
      ##
      engineName:

      ## Firebolt API endpoint. Used for authentication
      ##
      apiEndpoint:

    presto:
      ## The catalog within Presto to connect to
      ##
      catalog:

    elasticsearch:
      ## By default, queries return data in JDBC format, but you can also return data in standard Elasticsearch JDBC, JSON, CSV, YAML or raw formats (only JSON and JDBC are currently supported)
      ##
      queryFormat:

      ## If true, then use the Open Distro for Elasticsearch
      ##
      openDistro:

      ## ID of the API key from elastic.co
      ##
      apiKeyId:

      ## Value of the API key from elastic.co
      ##
      apiKeyKey:

    trino:
      ## The catalog within Trino to connect to
      ##
      catalog:
## Cubestore configuration
##
cubestore:
  ## The hostname of the Cube Store deployment
  ##
  host:

  ## The port of the Cube Store deployment
  ##
  port: 3030

api:
  ## Service account for cube api to use
  ##
  serviceAccount:
    create: false
    name: ""
    automountServiceAccountToken: true
    annotations: {}

  apiCount: 1

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Topology spread constraint for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  spreadConstraints: []

  ## Define resources requests and limits for single Pods.
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
  ## to match guidance: https://cube.dev/docs/product/deployment/production-checklist#appropriate-cluster-sizing
  # resources:
  #   requests:
  #     cpu: "2"
  #     memory: "3Gi"
  #   limits:
  #     cpu: "2"
  #     memory: "3Gi"

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ##  Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}

  ## Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}

  ## Tolerations for pod assignment
  ##
  tolerations: {}

  ## Node selector for pod assignment
  ##
  nodeSelector: {}

  ## Extra environment variables to pass on to the pod. The value is evaluated as a template
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  
  ## Name of a Config Map containing extra environment variables to pass on to the pod
  ##
  extraEnvVarsFromConfigMap:
  
  ## Name of a Secret containing extra environment variables to pass on to the pod
  ##
  extraEnvVarsFromSecret:

worker:
  ## Set to true to enable a separate refresh worker
  ##
  enabled: true

  ## Service account for cube worker to use
  ##
  serviceAccount:
    create: false
    name: ""
    automountServiceAccountToken: true
    annotations: {}

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## topology spread constraint for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  spreadConstraints: []

  ## Define resources requests and limits for single Pods.
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
  ## to match guidance: https://cube.dev/docs/product/deployment/production-checklist#appropriate-cluster-sizing
  # resources:
  #   requests:
  #     cpu: "2"
  #     memory: "6Gi"
  #   limits:
  #     cpu: "2"
  #     memory: "6Gi"
        
  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ## Configure options for liveness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  ##  Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}

  ## Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}

  ## Tolerations for pod assignment
  ##
  tolerations: {}

  ## Node selector for pod assignment
  ##
  nodeSelector: {}

  ## Extra environment variables to pass on to the pod. The value is evaluated as a template
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  
  ## Name of a Config Map containing extra environment variables to pass on to the pod
  ##
  extraEnvVarsFromConfigMap:
  
  ## Name of a Secret containing extra environment variables to pass on to the pod
  ##
  extraEnvVarsFromSecret:

## Configure the ingress resource that allows you to access the
## Cube installation. Set up the URL
## ref: http://kubernetes.io/docs/user-guide/ingress/
##
ingress:
  ## Set to true to enable ingress record generation
  ##
  enabled: false

  ## When the ingress is enabled, a host pointing to this will be created
  ##
  hostname: cube.local

  ## The Path to Cube. You may need to set this to '/*' in order to use this
  ## with ALB ingress controllers.
  ##
  path: /

  ## Ingress Path type
  ##
  pathType: ImplementationSpecific

  ## Ingress Class name
  ##
  ingressClassName:

  ## Ingress annotations done as key:value pairs
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ##
  annotations: {}

  ## Enable TLS configuration for the hostname defined at ingress.hostname parameter
  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}
  ## You can use the ingress.secrets parameter to create this TLS secret or relay on cert-manager to create it
  ##
  tls: false
